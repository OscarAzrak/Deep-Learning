{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "\n",
    "def LoadBatch(filename):\n",
    "    \"\"\" Copied from the dataset website \"\"\"\n",
    "    with open('Datasets/' + filename, 'rb') as fo:\n",
    "        dataset_dict = pickle.load(fo, encoding='bytes')\n",
    " \n",
    "    return dataset_dict\n",
    "\n",
    "def compute_grads_num_slow__(X, Y, net_params, lambda_, h):\n",
    "    L = len(net_params) // 2\n",
    "    grads = {}\n",
    "    for l in range(1, L + 1):\n",
    "        grads['W' + str(l)] = None\n",
    "        grads['b' + str(l)] = None\n",
    "\n",
    "    if net_params.get('use_bn'):\n",
    "        for l in range(1, L):\n",
    "            grads['gamma' + str(l)] = None\n",
    "            grads['beta' + str(l)] = None\n",
    "\n",
    "        \n",
    "    if net_params['use_bn']:\n",
    "        grads['gammas'] = [None] * len(net_params['gammas'])\n",
    "        grads['betas'] = [None] * len(net_params['betas'])\n",
    "    \n",
    "    for j in range(len(net_params['b'])):\n",
    "        grads['b'][j] = np.zeros_like(net_params['b'][j])\n",
    "        net_try = copy.deepcopy(net_params)\n",
    "        for i in range(len(net_params['b'][j])):\n",
    "            b_try = copy.deepcopy(net_params['b'])\n",
    "            b_try[j][i] -= h\n",
    "            net_try['b'] = b_try\n",
    "            c1 = compute_cost(X, Y, net_try, lambda_)\n",
    "            \n",
    "            b_try = copy.deepcopy(net_params['b'])\n",
    "            b_try[j][i] += h\n",
    "            net_try['b'] = b_try\n",
    "            c2 = compute_cost(X, Y, net_try, lambda_)\n",
    "            \n",
    "            grads['b'][j][i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "    for j in range(len(net_params['W'])):\n",
    "        grads['W'][j] = np.zeros_like(net_params['W'][j])\n",
    "        net_try = copy.deepcopy(net_params)\n",
    "        for i in np.ndindex(net_params['W'][j].shape):\n",
    "            W_try = copy.deepcopy(net_params['W'])\n",
    "            W_try[j][i] -= h\n",
    "            net_try['W'] = W_try\n",
    "            c1 = compute_cost(X, Y, net_try, lambda_)\n",
    "            \n",
    "            W_try = copy.deepcopy(net_params['W'])\n",
    "            W_try[j][i] += h\n",
    "            net_try['W'] = W_try\n",
    "            c2 = compute_cost(X, Y, net_try, lambda_)\n",
    "            \n",
    "            grads['W'][j][i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "    if net_params['use_bn']:\n",
    "        for j in range(len(net_params['gammas'])):\n",
    "            grads['gammas'][j] = np.zeros_like(net_params['gammas'][j])\n",
    "            net_try = copy.deepcopy(net_params)\n",
    "            for i in np.ndindex(net_params['gammas'][j].shape):\n",
    "                gammas_try = copy.deepcopy(net_params['gammas'])\n",
    "                gammas_try[j][i] -= h\n",
    "                net_try['gammas'] = gammas_try\n",
    "                c1 = compute_cost(X, Y, net_try, lambda_)\n",
    "\n",
    "                gammas_try = copy.deepcopy(net_params['gammas'])\n",
    "                gammas_try[j][i] += h\n",
    "                net_try['gammas'] = gammas_try\n",
    "                c2 = compute_cost(X, Y, net_try, lambda_)\n",
    "                \n",
    "                grads['gammas'][j][i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        for j in range(len(net_params['betas'])):\n",
    "            grads['betas'][j] = np.zeros_like(net_params['betas'][j])\n",
    "            net_try = copy.deepcopy(net_params)\n",
    "            for i in np.ndindex(net_params['betas'][j].shape):\n",
    "                betas_try = copy.deepcopy(net_params['betas'])  \n",
    "                betas_try[j][i] -= h\n",
    "                net_try['betas'] = betas_try\n",
    "                c1 = compute_cost(X, Y, net_try, lambda_)\n",
    "\n",
    "                betas_try = copy.deepcopy(net_params['betas'])\n",
    "                betas_try[j][i] += h\n",
    "                net_try['betas'] = betas_try\n",
    "                c2 = compute_cost(X, Y, net_try, lambda_)\n",
    "\n",
    "                grads['betas'][j][i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        \n",
    "    return grads\n",
    "\n",
    "\n",
    "def ComputeGradsNum(X, Y, P, W, b, lamda, h):\n",
    "\t\"\"\" Converted from matlab code \"\"\"\n",
    "\tno \t= \tW.shape[0]\n",
    "\td \t= \tX.shape[0]\n",
    "\n",
    "\t# grad_W = np.zeros(W.shape);\n",
    "\t# grad_b = np.zeros((no, 1));\n",
    "\n",
    "\tgrad_W = np.empty(shape=W.shape, dtype=object)\n",
    "\tgrad_b = np.empty(shape=b.shape, dtype=object)\n",
    "\n",
    "\tc = ComputeCost(X, Y, W, b, lamda)\n",
    "\t\n",
    "\tfor j in range(len(b)):\n",
    "\t\tgrad_b[j] = np.zeros(shape=(b[j].shape))\n",
    "\n",
    "\t\tfor i in range(len(b[j])):\n",
    "\t\t\tb_try = np.array(b)\n",
    "\t\t\tb_try[j][i] += h\n",
    "\t\t\tc2 = ComputeCost(X, Y, W, b_try, lamda)\n",
    "\t\t\tgrad_b[j][i] = (c2-c) / h\n",
    "\t\n",
    "\tfor k in range(len(W)):\n",
    "\t\tgrad_W[k] = np.zeros(shape=(W[k].shape))\n",
    "\n",
    "\t\tfor i in range(W[k].shape[0]):\n",
    "\t\t\tfor j in range(W[k].shape[1]):\n",
    "\t\t\t\tW_try = np.array(W)\n",
    "\t\t\t\tW_try[k][i,j] += h\n",
    "\t\t\t\tc2 = ComputeCost(X, Y, W_try, b, lamda)\n",
    "\n",
    "\t\t\t\tgrad_W[k][i,j] = (c2-c) / h\n",
    "\n",
    "\treturn [grad_W, grad_b]\n",
    "\n",
    "def compute_grads_num_slow(X, Y, net_params, lambda_, h):\n",
    "    L = len(net_params) // 2\n",
    "    grads = {}\n",
    "    for l in range(1, L + 1):\n",
    "        grads['W' + str(l)] = np.zeros_like(net_params['W' + str(l)])\n",
    "        grads['b' + str(l)] = np.zeros_like(net_params['b' + str(l)])\n",
    "\n",
    "    if net_params.get('use_bn'):\n",
    "        for l in range(1, L):\n",
    "            grads['gamma' + str(l)] = np.zeros_like(net_params['gamma' + str(l)])\n",
    "            grads['beta' + str(l)] = np.zeros_like(net_params['beta' + str(l)])\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        for i in np.ndindex(net_params['W' + str(l)].shape):\n",
    "            net_try = copy.deepcopy(net_params)\n",
    "            net_try['W' + str(l)][i] -= h\n",
    "            c1 = compute_cost(X, Y, net_try, lambda_)\n",
    "            \n",
    "            net_try = copy.deepcopy(net_params)\n",
    "            net_try['W' + str(l)][i] += h\n",
    "            c2 = compute_cost(X, Y, net_try, lambda_)\n",
    "            \n",
    "            grads['W' + str(l)][i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        for i in range(net_params['b' + str(l)].size):\n",
    "            net_try = copy.deepcopy(net_params)\n",
    "            net_try['b' + str(l)][i] -= h\n",
    "            c1 = compute_cost(X, Y, net_try, lambda_)\n",
    "            \n",
    "            net_try = copy.deepcopy(net_params)\n",
    "            net_try['b' + str(l)][i] += h\n",
    "            c2 = compute_cost(X, Y, net_try, lambda_)\n",
    "            \n",
    "            grads['b' + str(l)][i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "    if net_params['use_bn']:\n",
    "        for l in range(1, L):\n",
    "            for i in np.ndindex(net_params['gamma' + str(l)].shape):\n",
    "                net_try = copy.deepcopy(net_params)\n",
    "                net_try['gamma' + str(l)][i] -= h\n",
    "                c1 = compute_cost(X, Y, net_try, lambda_)\n",
    "\n",
    "                net_try = copy.deepcopy(net_params)\n",
    "                net_try['gamma' + str(l)][i] += h\n",
    "                c2 = compute_cost(X, Y, net_try, lambda_)\n",
    "                \n",
    "                grads['gamma' + str(l)][i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "            for i in np.ndindex(net_params['beta' + str(l)].shape):\n",
    "                net_try = copy.deepcopy(net_params)\n",
    "                net_try['beta' + str(l)][i] -= h\n",
    "                c1 = compute_cost(X, Y, net_try, lambda_)\n",
    "\n",
    "                net_try = copy.deepcopy(net_params)\n",
    "                net_try['beta' + str(l)][i] += h\n",
    "                c2 = compute_cost(X, Y, net_try, lambda_)\n",
    "\n",
    "                grads['beta' + str(l)][i] = (c2 - c1) / (2 * h)\n",
    "        \n",
    "    return grads\n",
    "\n",
    "\n",
    "def numerical_gradient(X, Y, parameters, lambda_):\n",
    "    h = 1e-6\n",
    "    G_Ws = []\n",
    "    G_bs = []\n",
    "    G_gammas = []\n",
    "    G_betas = []\n",
    "    c = compute_cost(X, Y, parameters, lambda_)\n",
    "\n",
    "    for idx, l in enumerate(parameters):\n",
    "        g_w = np.zeros_like(l[0])\n",
    "        g_b = np.zeros_like(l[1])\n",
    "        w_save = np.copy(l[0])\n",
    "        b_save = np.copy(l[1])\n",
    "\n",
    "        for i in range(len(l[1])):\n",
    "            l[1] = b_save.copy()\n",
    "            l[1][i] += h\n",
    "            c2 = compute_cost(X, Y, parameters, lambda_)\n",
    "            g_b[i] = (c2-c) / h\n",
    "\n",
    "        l[1] = b_save\n",
    "\n",
    "        for i in np.ndindex(l[0].shape):\n",
    "            l[0] = w_save.copy()\n",
    "            l[0][i] += h\n",
    "            c2 = compute_cost(X, Y, parameters, lambda_)\n",
    "            g_w[i] = (c2-c) / h\n",
    "\n",
    "        l[0] = w_save\n",
    "        G_Ws.append(g_w)\n",
    "        G_bs.append(g_b)\n",
    "\n",
    "        if parameters['use_bn'] and idx < len(parameters)-1:\n",
    "            g_gamma = np.zeros_like(l[2])\n",
    "            g_beta = np.zeros_like(l[3])\n",
    "            gamma_save = np.copy(l[2])\n",
    "            beta_save = np.copy(l[3])\n",
    "\n",
    "            for i in range(len(l[2])):\n",
    "                l[2] = gamma_save.copy()\n",
    "                l[2][i] += h\n",
    "                c2 = compute_cost(X, Y, parameters, lambda_)\n",
    "                g_gamma[i] = (c2 - c) / h\n",
    "\n",
    "            l[2] = gamma_save\n",
    "\n",
    "            for i in range(len(l[3])):\n",
    "                l[3] = beta_save.copy()\n",
    "                l[3][i] += h\n",
    "                c2 = compute_cost(X, Y, parameters, lambda_)\n",
    "                g_beta[i] = (c2 - c) / h\n",
    "\n",
    "            l[3] = beta_save\n",
    "            G_gammas.append(g_gamma)\n",
    "            G_betas.append(g_beta)\n",
    "\n",
    "    return G_Ws, G_bs, G_gammas, G_betas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def initialize_parameters(layer_dims):\\n    parameters = {}\\n    L = len(layer_dims)            # number of layers in the network\\n\\n    for l in range(1, L):\\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\\n        \\n    return parameters\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(filename):\n",
    "    data = LoadBatch(filename)\n",
    "    \n",
    "    X = data[b'data'].astype(np.float32).reshape(-1, 3072).T / 255\n",
    "    y = np.array(data[b'labels']).astype(np.int32)\n",
    "    \n",
    "    Y = np.zeros((10, y.shape[0]), dtype=np.float32)\n",
    "    for i, label in enumerate(y):\n",
    "        Y[label, i] = 1\n",
    "    \n",
    "    return X, Y, y + 1\n",
    "\n",
    "def normalize_data(train, validation, test):\n",
    "    # Compute the mean and standard deviation of the training data\n",
    "    train_mean = np.mean(train, axis=1, keepdims=True)\n",
    "    train_std = np.std(train, axis=1, keepdims=True)\n",
    "\n",
    "    # Normalize the training, validation and test data\n",
    "    train_norm = (train - train_mean) / train_std\n",
    "    validation_norm = (validation - train_mean) / train_std\n",
    "    test_norm = (test - train_mean) / train_std\n",
    "\n",
    "    return train_norm, validation_norm, test_norm\n",
    "\n",
    "\n",
    "# 3.1\n",
    "train_X, train_Y, train_y = load('data_batch_1')\n",
    "val_X, val_Y, val_y = load('data_batch_2')\n",
    "test_X, test_Y, test_y = load('test_batch')\n",
    "\n",
    "train_X_norm, val_X_norm, test_X_norm = normalize_data(train_X, val_X, test_X)\n",
    "# initialize parameters with output of parameters numpyy array with keys: 'W', 'b', 'gammas', 'betas', and 'use_bn'\n",
    "def initialize_parameters(layer_dims, use_bn=False):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.normal(0, 0.001, (layer_dims[l], layer_dims[l-1]))\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    if use_bn:\n",
    "        parameters['gammas'] = [np.ones((layer_dims[l], 1)) for l in range(1, L)]\n",
    "        parameters['betas'] = [np.zeros((layer_dims[l], 1)) for l in range(1, L)]\n",
    "        parameters['use_bn'] = True\n",
    "    else:\n",
    "        parameters['use_bn'] = False\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "\"\"\"def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    out = None\n",
    "    out = np.dot(w, x) + b\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "def relu_forward(x):\n",
    "    out = None\n",
    "    out = np.maximum(0, x)\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n",
    "def affine_relu_forward(x, w, b):\n",
    "    out, cache = None, None\n",
    "    a, fc_cache = affine_forward(x, w, b)\n",
    "    out, relu_cache = relu_forward(a)\n",
    "    cache = (fc_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "def compute_cost(X, Y, parameters, lambda_):\n",
    "    m = X.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # Forward propagation\n",
    "    A = X\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, _ = affine_relu_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)])\n",
    "    \n",
    "    scores, _ = affine_forward(A, parameters['W' + str(L)], parameters['b' + str(L)])\n",
    "    \n",
    "    # Compute cost\n",
    "    data_loss = np.sum(-np.log(np.sum(Y * scores, axis=0))) / m\n",
    "    reg_loss = 0\n",
    "    for l in range(1, L + 1):\n",
    "        reg_loss += np.sum(parameters['W' + str(l)] ** 2)\n",
    "    reg_loss *= lambda_ / (2 * m)\n",
    "    \n",
    "    return data_loss + reg_loss\n",
    "\n",
    "def compute_accuracy(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # Forward propagation\n",
    "    A = X\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, _ = affine_relu_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)])\n",
    "    \n",
    "    scores, _ = affine_forward(A, parameters['W' + str(L)], parameters['b' + str(L)])\n",
    "    \n",
    "    predicted_class = np.argmax(scores, axis=0)\n",
    "    return np.mean(predicted_class == (y - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7k/dhbxrqm54q998rq4v807swzr0000gn/T/ipykernel_57182/4227729906.py:33: RuntimeWarning: invalid value encountered in log\n",
      "  data_loss = np.sum(-np.log(np.sum(Y * scores, axis=0))) / m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), 'b1': array([[nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan]]), 'W2': array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]), 'b2': array([[nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan]])}\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters([3072, 50, 10], use_bn=False)\n",
    "grads = compute_grads_num_slow(train_X_norm[:, :100], train_Y[:, :100],parameters,lambda_=0, h=1e-6)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"def compute_gradients(X, Y, parameters, lambda_):\n",
    "    grads = {}\n",
    "    m = X.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # Forward propagation\n",
    "    A = X\n",
    "    A_cache = [None] * L\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = affine_relu_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)])\n",
    "        A_cache[l] = cache\n",
    "    \n",
    "    scores, cache = affine_forward(A, parameters['W' + str(L)], parameters['b' + str(L)])\n",
    "    \n",
    "    # Backpropagation\n",
    "    dA_prev = -Y / np.sum(Y * scores, axis=0)\n",
    "    dA, dW, db = affine_backward(dA_prev, cache)\n",
    "    grads['W' + str(L)] = dW + lambda_ * parameters['W' + str(L)] / m\n",
    "    grads['b' + str(L)] = db\n",
    "    \n",
    "    for l in reversed(range(1, L)):\n",
    "        dA_prev, dW, db = affine_relu_backward(dA, A_cache[l])\n",
    "        grads['W' + str(l)] = dW + lambda_ * parameters['W' + str(l)] / m\n",
    "        grads['b' + str(l)] = db\n",
    "    \n",
    "    return grads\"\"\"\n",
    "\n",
    "def compute_gradients(X, Y, parameters, lambda_):\n",
    "    grads = {}\n",
    "    m = X.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    # Forward propagation\n",
    "    A = X\n",
    "    A_cache = [None] * L\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = affine_relu_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)])\n",
    "        A_cache[l-1] = cache\n",
    "\n",
    "    scores, cache = affine_forward(A, parameters['W' + str(L)], parameters['b' + str(L)])\n",
    "\n",
    "    # Backpropagation\n",
    "    dA_prev = -Y / np.sum(Y * scores, axis=0)\n",
    "    dA, dW, db = affine_backward(dA_prev, cache)\n",
    "    grads['W' + str(L)] = dW + lambda_ * parameters['W' + str(L)] / m\n",
    "    grads['b' + str(L)] = db\n",
    "\n",
    "    for l in reversed(range(1, L)):\n",
    "        dA_prev, dW, db = affine_relu_backward(dA, A_cache[l-1])\n",
    "        grads['W' + str(l)] = dW + lambda_ * parameters['W' + str(l)] / m\n",
    "        grads['b' + str(l)] = db\n",
    "\n",
    "\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    dx = np.dot(w.T, dout)\n",
    "    dw = np.dot(dout, x.T)\n",
    "    db = np.sum(dout, axis=1, keepdims=True)\n",
    "    return dx, dw, db\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    dx, x = None, cache\n",
    "    dx = dout * (x > 0)\n",
    "    return dx\n",
    "\n",
    "def affine_relu_backward(dout, cache):\n",
    "    fc_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = affine_backward(da, fc_cache)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7k/dhbxrqm54q998rq4v807swzr0000gn/T/ipykernel_57182/4227729906.py:33: RuntimeWarning: invalid value encountered in log\n",
      "  data_loss = np.sum(-np.log(np.sum(Y * scores, axis=0))) / m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:\n",
      "\tW: mean=-19.660213502596363 std=481.46914056946076 mean_diff=nan std_diff=nan max_diff=nan\n",
      "\tb: mean=-119.33349425135877 std=510.2957225637157 mean_diff=nan std_diff=nan max_diff=nan\n",
      "Layer 2:\n",
      "\tW: mean=-3930.3092677894383 std=8187.963736351885 mean_diff=nan std_diff=nan max_diff=nan\n",
      "\tb: mean=-189781.84195733376 std=207836.86097300384 mean_diff=nan std_diff=nan max_diff=nan\n"
     ]
    }
   ],
   "source": [
    "# compare gradients\n",
    "def compare_gradients(X, Y, parameters, lambda_):\n",
    "    L = len(parameters) // 2\n",
    "    # Compute gradients using backpropagation\n",
    "    grads = compute_gradients(X, Y, parameters, lambda_)\n",
    "    \n",
    "    # Compute gradients using numerical gradient approximation\n",
    "    num_grads = compute_grads_num_slow(X, Y, parameters, lambda_, h=1e-6)\n",
    "    \n",
    "    # Loop over each layer and compare gradients\n",
    "    for l in range(1, L+1):\n",
    "        print(f'Layer {l}:')\n",
    "        grad_w = np.array(grads['W' + str(l)])\n",
    "        num_grad_w = np.array(num_grads['W' + str(l)])\n",
    "        diff_w = num_grad_w - grad_w\n",
    "        print(f'\\tW: mean={np.mean(grad_w)} std={np.std(grad_w)} mean_diff={np.mean(diff_w)} std_diff={np.std(diff_w)} max_diff={np.max(diff_w)}')\n",
    "\n",
    "        grad_b = np.array(grads['b' + str(l)])\n",
    "        num_grad_b = np.array(num_grads['b' + str(l)])\n",
    "        diff_b = num_grad_b - grad_b\n",
    "        print(f'\\tb: mean={np.mean(grad_b)} std={np.std(grad_b)} mean_diff={np.mean(diff_b)} std_diff={np.std(diff_b)} max_diff={np.max(diff_b)}')\n",
    "    \n",
    "    return grads, num_grads\n",
    "\n",
    "\n",
    "\n",
    "# compare gradients, layers: 2\n",
    "parameters2 = initialize_parameters([3072, 50, 10], use_bn=False)\n",
    "grads2, num_grads2 = compare_gradients(train_X_norm[:, :100], train_Y[:, :100], parameters2, lambda_=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W2': array([[ 5.48218804e+02,  2.20135106e+02,  3.63533054e+03,\n",
      "         5.09108937e+01,  2.15873855e+03,  4.65056262e+01,\n",
      "         1.01052309e+03,  1.36199080e+03,  4.92376163e+02,\n",
      "         6.53963162e+02, -2.82600413e+02, -7.69621593e+01,\n",
      "         1.61074043e+02, -1.52912856e+01, -7.33006352e+01,\n",
      "         3.35400078e+02, -3.29425417e+02, -4.08817668e+02,\n",
      "         1.25052467e+03, -1.64882603e+02, -3.55493967e+02,\n",
      "         1.68592819e+03,  1.43686931e+02,  8.96924357e+02,\n",
      "        -2.69653700e+02,  1.45107918e+02, -6.18293146e+01,\n",
      "        -2.73035453e+02, -2.60231276e+02,  2.94054749e+02,\n",
      "         1.24860620e+02, -3.48093291e+02,  1.77242238e+03,\n",
      "        -7.07680058e+02,  4.84974124e+02, -2.28554037e+01,\n",
      "         1.86368327e+02, -2.36158223e+02, -1.25637355e+03,\n",
      "        -4.90382276e+02, -2.62568261e+02, -8.42922397e+02,\n",
      "         1.17645355e+03,  5.47520788e+02,  4.27594148e+02,\n",
      "         4.75610688e+02,  1.93543303e+02,  3.51817643e+02,\n",
      "         5.73964769e+02, -2.20129525e+02],\n",
      "       [-5.34413682e+04, -2.46742450e+02,  2.06251946e+02,\n",
      "        -5.29149884e+04, -1.17703931e+04, -2.84065708e+04,\n",
      "        -5.27793218e+04, -2.41529725e+03,  1.27824155e+03,\n",
      "         1.15676160e+03,  8.79679852e+02,  1.26258477e+02,\n",
      "        -6.03519910e+04,  3.99204747e+02,  2.41207100e+03,\n",
      "         6.99465702e+02, -2.34877219e+04,  4.96442680e+02,\n",
      "         1.69244834e+03, -1.00845237e+03, -1.10589387e+02,\n",
      "        -1.72787061e+04, -3.00731405e+03,  5.22169736e+02,\n",
      "        -3.13302191e+03, -6.24723532e+04, -1.50942298e+03,\n",
      "         3.44389813e+03,  2.11784763e+03, -2.46764295e+03,\n",
      "        -5.76523349e+04, -6.51277063e+03,  1.48620601e+02,\n",
      "        -5.48301735e+02, -9.74241516e+03, -5.15763669e+01,\n",
      "        -3.62688220e+04,  2.33392883e+03, -3.22187939e+03,\n",
      "        -3.63107760e+04,  3.92027736e+02,  1.97923095e+03,\n",
      "         1.59035912e+03,  9.73378697e+02,  1.17952956e+03,\n",
      "        -1.38341995e+04,  5.84758670e+02,  3.20826636e+02,\n",
      "        -1.39452608e+03, -6.55652456e+04],\n",
      "       [ 8.56938403e+02,  1.16011979e+03,  1.95656004e+03,\n",
      "         4.21229272e+02,  1.28470187e+03,  2.64830860e+03,\n",
      "         2.94127911e+02,  2.90226478e+03,  2.05629336e+03,\n",
      "         1.12604707e+03,  2.71977478e+03,  2.16593784e+03,\n",
      "         3.79811081e+03,  2.13783195e+03,  1.47557327e+03,\n",
      "         2.05166728e+03,  2.36109132e+03,  8.64008653e+02,\n",
      "         4.09888777e+03,  2.47013286e+03,  9.20046299e+02,\n",
      "         9.47891030e+02,  2.08070959e+03,  8.56935578e+02,\n",
      "         9.83516715e+02,  7.00128888e+03,  9.65984820e+02,\n",
      "         1.30011217e+03,  1.98608480e+03,  4.91463413e+03,\n",
      "         1.74466064e+02,  1.81262835e+03,  3.25679585e+03,\n",
      "         6.39480997e+03,  1.56160749e+03,  6.17919536e+02,\n",
      "         5.35053932e+03,  1.12841229e+04,  9.17678899e+03,\n",
      "         1.58334233e+03,  8.52935908e+03,  1.81318931e+03,\n",
      "         1.53871324e+03,  1.50894826e+03,  2.37694294e+03,\n",
      "         6.68624895e+02,  8.58606665e+02,  1.66919502e+03,\n",
      "         3.31476029e+03,  1.02680106e+03],\n",
      "       [ 1.44085125e+02,  3.09294735e+02,  4.25280440e+02,\n",
      "        -2.21454630e+03,  6.33036067e+02, -6.36904445e+02,\n",
      "        -3.11942152e+01, -7.16535528e+02, -5.28662490e+03,\n",
      "        -3.15645140e+03,  5.76202820e+02,  8.51041372e+01,\n",
      "        -4.67621147e+02, -2.90354485e+03, -4.05272899e+03,\n",
      "        -1.27946577e+03,  8.24204487e+01,  8.74078723e+02,\n",
      "        -1.71296206e+03, -2.92525798e+02, -8.95788485e+02,\n",
      "         4.51615738e+02, -1.43933407e+03, -2.03473045e+03,\n",
      "        -5.01585885e+03, -2.12582541e+02, -2.08680607e+03,\n",
      "        -6.06037612e+03, -3.69782058e+03, -5.28328119e+03,\n",
      "        -1.89790214e+02, -8.00655006e+02, -5.15602252e+02,\n",
      "        -1.26049552e+03, -3.67870609e+03, -9.05502005e+01,\n",
      "        -3.30227933e+03, -5.58972895e+02, -4.86458687e+03,\n",
      "        -7.81535180e+03, -4.96689225e+03, -9.62357266e+03,\n",
      "         6.82493055e+02, -1.45154581e+03, -5.80442869e+03,\n",
      "        -2.00384843e+02, -3.71536606e+02,  3.87210238e+02,\n",
      "        -2.02560513e+02, -2.28664243e+03],\n",
      "       [-9.57914721e+02, -4.85205112e+02, -1.81031192e+02,\n",
      "        -4.97421698e+02, -1.82559458e+02, -3.85114639e+02,\n",
      "        -5.57394207e+02, -4.33087746e+02, -5.91632145e+02,\n",
      "         4.79450423e+02, -3.87595909e+02, -5.72128203e+02,\n",
      "        -9.74366729e+02, -2.82990325e+02, -4.01710453e+02,\n",
      "        -6.23403281e+02, -1.27099918e+02,  1.33373462e+03,\n",
      "        -9.53850855e+02,  9.61665792e+02, -9.43334298e+02,\n",
      "        -4.50007200e+02, -7.99001709e+02, -1.51342487e+02,\n",
      "        -7.91679226e+02, -3.78165727e+02, -1.79046130e+02,\n",
      "        -1.29578698e+03, -1.89544321e+02, -8.24833564e+02,\n",
      "         4.02146008e+02, -4.18306068e+02, -1.29559363e+03,\n",
      "        -1.22356065e+03,  6.21479439e+02,  8.95986440e+01,\n",
      "        -5.10289645e+02,  5.44658965e+02, -9.66925954e+02,\n",
      "        -1.21496952e+03, -1.00036638e+03, -2.61982981e+02,\n",
      "        -8.43082209e+02, -5.09414871e+02,  1.47803992e+03,\n",
      "         2.36181810e+02, -2.57730562e+02, -3.78121085e+02,\n",
      "        -7.06201662e+02, -3.37619564e+02],\n",
      "       [ 9.66116871e+02,  2.38293232e+02,  3.32612581e+02,\n",
      "        -1.39737739e+02,  1.56972156e+03,  1.19860214e+03,\n",
      "         1.99439161e+02, -5.63166189e+01, -4.93731425e+02,\n",
      "         1.38746550e+03, -8.32660486e+02,  1.84719746e+02,\n",
      "         6.67496474e+02,  2.44271046e+02, -2.53910160e+01,\n",
      "        -2.66751986e+02,  5.91890946e+02, -1.00596361e+03,\n",
      "         9.19672862e+02,  7.36203824e+02,  1.90033186e+02,\n",
      "         4.59047265e+01,  3.95680508e+02,  2.85404163e+02,\n",
      "         2.07494295e+02,  5.96721437e+02,  1.31626124e+02,\n",
      "         7.79956882e+01,  0.00000000e+00,  4.67076422e+02,\n",
      "         7.85815372e+02,  2.13223014e+02,  4.00665367e+02,\n",
      "         2.74746079e+02,  1.46891274e+02,  4.41032529e+02,\n",
      "        -9.17765975e+02,  1.08218519e+03, -4.47283522e+02,\n",
      "         2.00767887e+02,  4.66574182e+02,  2.82660058e+02,\n",
      "         3.88834625e+02, -1.41434042e+03,  1.05411810e+03,\n",
      "         1.44314926e+02,  5.69256085e+01,  6.28858912e+02,\n",
      "         3.29420493e+02,  7.77412868e+02],\n",
      "       [ 7.46341235e+02,  4.50081399e+02,  1.41086418e+03,\n",
      "         3.82197910e+02,  9.82905927e+02,  3.82722488e+02,\n",
      "         5.20458579e+02,  1.23143363e+02,  6.41463935e+02,\n",
      "         6.88580053e+02,  3.56627270e+02,  1.32193378e+03,\n",
      "         6.13181186e+02,  2.69124672e+02,  5.10147940e+02,\n",
      "         5.84016294e+02,  2.86481405e+02,  5.84671969e+02,\n",
      "         1.07092965e+02,  5.58275859e+02,  1.07898713e+02,\n",
      "         8.08216218e+02,  5.22443301e+02,  6.89430375e+02,\n",
      "         6.01190498e+02,  6.69820759e+02,  4.10547635e+02,\n",
      "         4.89635438e+02,  1.21140619e+02,  6.14193062e+02,\n",
      "         6.67102351e+02,  2.63046817e+02,  8.47402401e+02,\n",
      "         4.08467931e+02,  1.63513954e+02,  5.44373628e+02,\n",
      "         6.34480118e+02,  7.06210753e+02,  6.87487171e+02,\n",
      "         6.57406184e+02,  5.26070168e+02,  8.67202220e+02,\n",
      "         5.82784693e+02,  7.54520853e+02,  3.61534906e+02,\n",
      "         7.57205713e+02,  7.83563411e+02,  7.26445915e+02,\n",
      "         5.52161933e+02,  2.62447098e+02],\n",
      "       [ 4.88055402e+03,  2.92317932e+03,  2.19026529e+02,\n",
      "         2.86405343e+02,  3.98738395e+03,  2.90733887e+03,\n",
      "        -7.39713349e+02,  1.76181398e+03, -1.58092647e+03,\n",
      "        -2.67305225e+02, -9.43035013e+02,  1.04119601e+04,\n",
      "         6.36171756e+02, -1.43404152e+03, -1.95260721e+03,\n",
      "         3.07556620e+03, -2.20351652e+03, -6.47962751e+02,\n",
      "         2.92366297e+03, -1.20066902e+03,  1.92140216e+03,\n",
      "         1.09587348e+03, -9.13780679e+02,  3.33922029e+02,\n",
      "        -1.86832260e+03, -2.11425054e+03, -6.02704039e+02,\n",
      "        -5.37965486e+02, -3.94968161e+02, -1.72390930e+03,\n",
      "        -6.15607727e+02, -1.67910402e+03, -3.66802103e+03,\n",
      "        -2.47161225e+03,  4.98614162e+03, -8.50401683e+02,\n",
      "        -3.14542222e+02, -6.32050280e+03, -6.53599555e+02,\n",
      "         2.77612360e+02, -1.63995593e+03, -1.12999664e+03,\n",
      "         4.21140555e+03,  3.46407572e+03, -3.21219057e+03,\n",
      "         1.44733124e+03,  1.58659226e+03,  1.46075295e+03,\n",
      "        -2.15918512e+03, -5.56610188e+00],\n",
      "       [ 1.39152689e+02, -1.40325173e+03, -1.42575844e+03,\n",
      "         2.05556322e+02,  1.00200020e+02,  3.46920211e+02,\n",
      "         2.94501227e+02, -6.36731801e+02, -1.69044462e+03,\n",
      "         8.02817270e+01,  1.92034405e+02, -2.55873075e+02,\n",
      "        -7.68676446e+02, -5.08277620e+01, -4.14157948e+02,\n",
      "        -1.45262943e+02,  4.45371765e+01, -7.12587321e+01,\n",
      "         1.77146011e+02, -9.55319939e+01, -4.92872272e+02,\n",
      "        -3.72671456e+02, -2.06175086e+01, -1.75860670e+03,\n",
      "        -2.31173081e+02,  1.33844677e+02,  1.19803331e+02,\n",
      "        -1.43088302e+02, -2.00804451e+03,  8.42386078e+01,\n",
      "         2.92884380e+02, -1.31361929e+02,  5.08925699e+01,\n",
      "         9.54646834e+01, -9.96546639e+02, -2.55058002e+03,\n",
      "        -1.29426105e+02,  0.00000000e+00,  2.38422183e+02,\n",
      "         7.70636963e+02,  0.00000000e+00, -8.01298070e+01,\n",
      "         1.18405928e+02, -1.56887651e+03, -1.29562773e+03,\n",
      "         1.31567117e+02, -1.51946050e+03, -7.62536281e+02,\n",
      "        -8.47127584e+01,  8.40386937e+02],\n",
      "       [-6.50833676e+03, -3.52356906e+03, -8.00238360e+01,\n",
      "        -8.85087761e+02, -9.51971968e+03, -4.06088501e+03,\n",
      "        -1.20317238e+04, -7.47883954e+03, -1.46791884e+03,\n",
      "        -1.56113278e+03, -2.92536211e+03, -3.21785517e+03,\n",
      "        -2.62715155e+04, -1.15305089e+04, -6.92019260e+03,\n",
      "        -5.46732345e+02, -2.26823628e+02, -1.36889693e+04,\n",
      "        -1.91275913e+04, -1.82022707e+04, -7.77563927e+03,\n",
      "        -4.61947506e+03, -6.15692778e+02, -3.15199102e+03,\n",
      "        -1.81594293e+03, -1.82075215e+04, -6.37250093e+03,\n",
      "        -2.14191797e+03, -3.22425589e+03, -6.36419185e+03,\n",
      "         2.13121356e+02, -8.39347264e+03, -2.21659437e+03,\n",
      "        -1.56653286e+04, -1.11670319e+04, -1.58514426e+03,\n",
      "        -1.42838899e+04, -1.23573260e+04, -1.29021216e+03,\n",
      "        -1.27551778e+03, -7.70041015e+03, -1.14641751e+02,\n",
      "        -1.90736963e+04, -2.01755117e+03, -1.39083012e+03,\n",
      "        -5.56777966e+02, -2.09189859e+03, -2.22794153e+03,\n",
      "        -7.34667012e+03, -1.21491534e+04]]), 'b2': array([[ 2.21056315e+04],\n",
      "       [-8.50372551e+05],\n",
      "       [ 1.28328517e+05],\n",
      "       [-7.52289106e+04],\n",
      "       [-1.47706411e+04],\n",
      "       [ 9.98327212e+03],\n",
      "       [ 2.77571338e+04],\n",
      "       [ 5.86055363e+02],\n",
      "       [-5.87898435e+03],\n",
      "       [-2.80801010e+05]]), 'W1': array([[1138.39560158,  363.44279021,  541.12687465, ..., -410.52760332,\n",
      "        -379.38264653, -380.89615087],\n",
      "       [   4.8343587 ,  -13.57028738,  -56.39056658, ...,   28.79570967,\n",
      "          20.16595915,   23.43504512],\n",
      "       [-106.01566553,  -83.59245321,  -91.29306561, ...,  152.24910129,\n",
      "         162.25427176,  155.06669925],\n",
      "       ...,\n",
      "       [-104.02986608, -133.77589772, -139.7357688 , ...,   69.62320291,\n",
      "          81.00082573,   58.11257259],\n",
      "       [-295.74578585, -295.9480707 , -275.71733871, ...,  -19.07520115,\n",
      "         -29.55637841,  -15.63693392],\n",
      "       [-566.23625871,  -36.68344664, -123.22931118, ...,  392.57669348,\n",
      "         385.82215742,  326.24533376]]), 'b1': array([[  890.54705363],\n",
      "       [  -67.24004601],\n",
      "       [   35.0249852 ],\n",
      "       [-1571.42232858],\n",
      "       [  830.32566351],\n",
      "       [  634.72230018],\n",
      "       [  -82.14445543],\n",
      "       [  -96.22907174],\n",
      "       [  -47.72599194],\n",
      "       [  -85.52956065],\n",
      "       [  -73.9487707 ],\n",
      "       [  -65.60114936],\n",
      "       [ -559.07374605],\n",
      "       [   78.5954476 ],\n",
      "       [ -691.36581749],\n",
      "       [    8.58103762],\n",
      "       [  396.31537909],\n",
      "       [ -589.61903434],\n",
      "       [  -43.61449011],\n",
      "       [   45.88350548],\n",
      "       [  -47.2193937 ],\n",
      "       [  724.02667113],\n",
      "       [ -318.67086154],\n",
      "       [   19.88946856],\n",
      "       [ 1901.84784975],\n",
      "       [ -459.61057238],\n",
      "       [  102.09182625],\n",
      "       [  222.72255452],\n",
      "       [  -40.9360457 ],\n",
      "       [ -174.77359553],\n",
      "       [  184.38065906],\n",
      "       [ 2444.9010558 ],\n",
      "       [   88.45272348],\n",
      "       [ -154.58903771],\n",
      "       [  320.29026731],\n",
      "       [  -59.66922069],\n",
      "       [ 1685.68042234],\n",
      "       [  304.61806803],\n",
      "       [ -171.56209898],\n",
      "       [ -601.15839062],\n",
      "       [  -53.9516555 ],\n",
      "       [   17.92762841],\n",
      "       [   70.20972742],\n",
      "       [ -373.25299259],\n",
      "       [  138.99452287],\n",
      "       [  577.62222678],\n",
      "       [   76.52940172],\n",
      "       [  195.60260203],\n",
      "       [  147.6645597 ],\n",
      "       [ -463.33536673]])}\n"
     ]
    }
   ],
   "source": [
    "print(grads2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compare gradients, layers: 3\n",
    "parameters3 = initialize_parameters([3072, 50, 50, 10], use_bn=False)\n",
    "grads3, num_grads3 = compare_gradients(train_X_norm[:, :100], train_Y[:, :100], parameters3, lambda_=0)\n",
    "\n",
    "# compare gradients, layers: 4\n",
    "parameters4 = initialize_parameters([3072, 50, 50, 50, 10], use_bn=False)\n",
    "grads4, num_grads4 = compare_gradients(train_X_norm[:, :100], train_Y[:, :100], parameters4, lambda_=0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
