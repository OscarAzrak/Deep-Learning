{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 55, 62, 62, 65]\n",
      "['h', 'e', 'l', 'l', 'o']\n"
     ]
    }
   ],
   "source": [
    "# Reading the text file\n",
    "with open('data/goblet_book.txt', 'r') as file:\n",
    "    book_data = file.read()\n",
    "\n",
    "# Getting unique characters\n",
    "book_chars = sorted(set(book_data))\n",
    "K = len(book_chars)  # dimensionality of the output (input) vector of your RNN\n",
    "\n",
    "# Initializing maps\n",
    "\n",
    "char_to_ind = {char: ind for ind, char in enumerate(book_chars)}\n",
    "ind_to_char = {ind: char for ind, char in enumerate(book_chars)}\n",
    "\n",
    "\n",
    "\n",
    "# Now, char_to_ind and ind_to_char can be used for converting between characters and their corresponding indices\n",
    "\n",
    "\n",
    "# test using text \"hello\"\n",
    "hello = 'hello'\n",
    "hello_ind = [char_to_ind[char] for char in hello]\n",
    "print(hello_ind)\n",
    "print([ind_to_char[ind] for ind in hello_ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set hyper-parameters\n",
    "m = 100  # dimensionality of the hidden state\n",
    "eta = 0.1  # learning rate\n",
    "seq_length = 25  # length of the input sequences\n",
    "\n",
    "# Initialize the RNN's parameters\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1, seq_length=25, sigma=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(hidden_size, hidden_size) / np.sqrt(hidden_size)  # weights for hidden states\n",
    "        self.U = np.random.randn(hidden_size, input_size) / np.sqrt(input_size)  # weights for inputs\n",
    "        self.b = np.zeros((hidden_size, 1))                        # bias for hidden states\n",
    "        self.V = np.random.randn(output_size, hidden_size) / np.sqrt(hidden_size) # weights for output\n",
    "        self.c = np.zeros((output_size, 1))                        # bias for output\n",
    "\n",
    "\n",
    "m = 100\n",
    "eta = 0.1\n",
    "seq_length = 25\n",
    "\n",
    "# Initialize the RNN\n",
    "rnn = RNN(K, m, K, learning_rate=eta, seq_length=seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.iDk9apKwVB})w(ün1\tk-nqwNN•iYiURM;S0Y79OKYu/i0}kx)h/RLXr,fVPCm•e(.rDjuct'W6,Cdv(U\n",
      "eKkMN- cIu\n",
      "olp03F\n"
     ]
    }
   ],
   "source": [
    "def synthesize(rnn, h0, x0, n):\n",
    "    h = h0\n",
    "    x = x0\n",
    "    Y = np.zeros((rnn.output_size, n))\n",
    "\n",
    "\n",
    "    for t in range(n):\n",
    "        # Compute the hidden state\n",
    "        a = np.dot(rnn.W, h.ravel()) + np.dot(rnn.U, x.ravel()) + rnn.b.ravel()\n",
    "        h = np.tanh(a)\n",
    "\n",
    "        # Compute the output\n",
    "        o = np.dot(rnn.V, h) + rnn.c.ravel()\n",
    "        p = np.exp(o) / np.sum(np.exp(o))  # normalize to get probabilities\n",
    "\n",
    "        # Sample a character index from the probability distribution\n",
    "        ix = np.random.choice(range(rnn.output_size), p=p.ravel())\n",
    "\n",
    "        # Update the input for the next time step\n",
    "        x = np.zeros((rnn.input_size, 1))\n",
    "        x[ix] = 1\n",
    "\n",
    "        # Store the one-hot representation of the sampled character\n",
    "        Y[:, t] = x.ravel()\n",
    "\n",
    "\n",
    "    return Y\n",
    "\n",
    "def one_hot_seq_to_char_seq(one_hot_seq, ind_to_char):\n",
    "    N = one_hot_seq.shape[1]\n",
    "    char_seq = ''.join([ind_to_char[np.argmax(one_hot_seq[:, i])] for i in range(N)])\n",
    "    return char_seq\n",
    "\n",
    "h0 = np.zeros((m, 1))\n",
    "x0 = np.zeros((K, 1))\n",
    "n = 100\n",
    "\n",
    "Y = synthesize(rnn, h0, x0, n)\n",
    "#text = ''.join(ind_to_char[np.argmax(Y[:, i])] for i in range(n))\n",
    "text = one_hot_seq_to_char_seq(Y, ind_to_char)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_backward_pass(rnn, X_chars, Y_chars, h0):\n",
    "    # Convert the characters to one-hot encodings\n",
    "    X = np.zeros((len(rnn.c), len(X_chars)))\n",
    "    Y = np.zeros((len(rnn.c), len(Y_chars)))\n",
    "    for t in range(len(X_chars)):\n",
    "        X[char_to_ind[X_chars[t]], t] = 1\n",
    "        Y[char_to_ind[Y_chars[t]], t] = 1\n",
    "\n",
    "    # Initialize the hidden states and outputs\n",
    "    h = np.zeros((rnn.b.shape[0], len(X_chars) + 1))\n",
    "    h[:, 0] = h0.ravel()\n",
    "    o = np.zeros((len(rnn.c), len(X_chars)))\n",
    "\n",
    "    # Forward pass\n",
    "    for t in range(len(X_chars)):\n",
    "        a1 = np.dot(rnn.W, h[:, t]) \n",
    "        a2 = np.dot(rnn.U, X[:, t]) \n",
    "\n",
    "        rnn.b = rnn.b.ravel()\n",
    "        a = a1 + a2 + rnn.b\n",
    "\n",
    "        h[:, t + 1] = np.tanh(a).ravel()\n",
    "\n",
    "        rnn.c = rnn.c.ravel()\n",
    "\n",
    "        o[:, t] = (np.dot(rnn.V, h[:, t + 1]) + rnn.c).ravel()\n",
    "\n",
    "    p = np.exp(o) / np.sum(np.exp(o), axis=0)  # normalize to get probabilities\n",
    "    loss = -np.sum(Y * np.log(p))  # cross-entropy loss\n",
    "\n",
    "    # Initialize the gradients\n",
    "    grads = {\n",
    "        'U': np.zeros_like(rnn.U),\n",
    "        'W': np.zeros_like(rnn.W),\n",
    "        'V': np.zeros_like(rnn.V),\n",
    "        'b': np.zeros_like(rnn.b),\n",
    "        'c': np.zeros_like(rnn.c)\n",
    "    }\n",
    "\n",
    "    # Backward pass\n",
    "    dh_next = np.zeros_like(h[:, 0])\n",
    "    for t in reversed(range(len(X_chars))):\n",
    "        do = p[:, t] - Y[:, t]\n",
    "        grads['V'] += np.outer(do, h[:, t + 1])\n",
    "        grads['c'] += do.reshape(rnn.c.shape)\n",
    "\n",
    "        dh = np.dot(rnn.V.T, do) + dh_next\n",
    "        da = (1 - h[:, t + 1]**2) * dh\n",
    "\n",
    "        grads['U'] += np.outer(da, X[:, t])\n",
    "        grads['W'] += np.outer(da, h[:, t])\n",
    "        grads['b'] += da.reshape(rnn.b.shape)\n",
    "\n",
    "        dh_next = np.dot(rnn.W.T, da)\n",
    "\n",
    "    # Clip the gradients to avoid exploding gradient problem\n",
    "    for grad in grads.values():\n",
    "        np.clip(grad, -5, 5, out=grad)\n",
    "\n",
    "    return loss, grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARRY POTTER AND THE GOBL\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pri' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m X_ \u001b[39m=\u001b[39m char_seq_to_one_hot(X_chars, char_to_ind, K)\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(X_chars_encoded)\n\u001b[0;32m---> 28\u001b[0m pri(\u001b[39m\"\u001b[39m\u001b[39m________\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(X_)\n\u001b[1;32m     33\u001b[0m loss, grads \u001b[39m=\u001b[39m forward_backward_pass(rnn, X_chars, Y_chars, h0)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pri' is not defined"
     ]
    }
   ],
   "source": [
    "X_chars = book_data[0:seq_length]\n",
    "Y_chars = book_data[1:seq_length + 1]\n",
    "\n",
    "\n",
    "def one_hot_encode(sequence, K):\n",
    "    # Create an identity matrix of size K\n",
    "    I = np.eye(K)\n",
    "\n",
    "    # Convert the sequence to a list of one-hot encoded vectors\n",
    "    encoded_sequence = np.array([I[char_to_ind[ch]] for ch in sequence])\n",
    "\n",
    "    return encoded_sequence\n",
    "\n",
    "# One-hot encode the input sequences\n",
    "X_chars_encoded = one_hot_encode(X_chars, K)\n",
    "Y_chars_encoded = one_hot_encode(Y_chars, K)\n",
    "\n",
    "print(X_chars)\n",
    "\n",
    "def char_seq_to_one_hot(char_seq, char_to_ind, K):\n",
    "    N = len(char_seq)\n",
    "    one_hot_seq = np.zeros((K, N))\n",
    "    for i, char in enumerate(char_seq):\n",
    "        one_hot_seq[char_to_ind[char], i] = 1\n",
    "    return one_hot_seq\n",
    "X_ = char_seq_to_one_hot(X_chars, char_to_ind, K)\n",
    "print(X_chars_encoded)\n",
    "print(\"________\")\n",
    "print(X_)\n",
    "\n",
    "\n",
    "\n",
    "loss, grads = forward_backward_pass(rnn, X_chars, Y_chars, h0)\n",
    "for param, grad in grads.items():\n",
    "    param_value = getattr(rnn, param)\n",
    "    param_value -= eta * grad\n",
    "    setattr(rnn, param, param_value)\n",
    "\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RNN2' object has no attribute 'seq_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSynthesized text:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, synthesized_seq)\n\u001b[1;32m     43\u001b[0m         e \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m seq_length\n\u001b[0;32m---> 44\u001b[0m rnn \u001b[39m=\u001b[39m train_rnn(rnn, book_data, char_to_ind, ind_to_char, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m7\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[113], line 4\u001b[0m, in \u001b[0;36mtrain_rnn\u001b[0;34m(rnn, book_data, char_to_ind, ind_to_char, n_epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_rnn\u001b[39m(rnn, book_data, char_to_ind, ind_to_char, n_epochs):\n\u001b[1;32m      3\u001b[0m     K \u001b[39m=\u001b[39m rnn\u001b[39m.\u001b[39moutput_size\n\u001b[0;32m----> 4\u001b[0m     seq_length \u001b[39m=\u001b[39m rnn\u001b[39m.\u001b[39;49mseq_length\n\u001b[1;32m      6\u001b[0m     iter_per_epoch \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(book_data) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m seq_length\n\u001b[1;32m      7\u001b[0m     updates \u001b[39m=\u001b[39m n_epochs \u001b[39m*\u001b[39m iter_per_epoch\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RNN2' object has no attribute 'seq_length'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def train_rnn(rnn, book_data, char_to_ind, ind_to_char, n_epochs):\n",
    "    K = rnn.output_size\n",
    "    seq_length = rnn.seq_length\n",
    "\n",
    "    iter_per_epoch = len(book_data) // seq_length\n",
    "    updates = n_epochs * iter_per_epoch\n",
    "    smooth_loss = -np.log(1.0 / K) * seq_length  # loss at iteration 0\n",
    "    hprev = np.zeros((rnn.hidden_size, 1))\n",
    "    ada_params = {k: np.zeros_like(getattr(rnn, k)) for k in ['U', 'W', 'V']}\n",
    "\n",
    "    e= 0\n",
    "    for update in tqdm(range(updates)):\n",
    "        if e == 0 or e + seq_length + 1 > len(book_data):\n",
    "            e = 1\n",
    "            hprev = np.zeros((rnn.hidden_size, 1))  # reset RNN memory\n",
    "\n",
    "        X_chars = book_data[e:e + seq_length]\n",
    "        Y_chars = book_data[e + 1:e + seq_length + 1]\n",
    "\n",
    "        X = one_hot_encode(X_chars, K)\n",
    "        Y = one_hot_encode(Y_chars, K)\n",
    "\n",
    "        loss, grads = forward_backward_pass(rnn, X, Y, hprev)\n",
    "\n",
    "        # Update the parameters using Adagrad\n",
    "        for param, grad in grads.items():\n",
    "            param_value = getattr(rnn, param)\n",
    "            ada_params[param] += grad**2\n",
    "            param_value -= eta * grad / np.sqrt(ada_params[param] + 1e-8)\n",
    "            setattr(rnn, param, param_value)\n",
    "\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "        if update % 10000 == 0:\n",
    "            print(\"Smooth loss at step {}: {}\".format(update, smooth_loss))\n",
    "\n",
    "        if update % 100000 == 0:\n",
    "            Y_synthesized = synthesize(rnn, hprev, X[:, :1], 200)\n",
    "            synthesized_seq = one_hot_seq_to_char_seq(Y_synthesized, ind_to_char)\n",
    "            print(\"Synthesized text:\\n\", synthesized_seq)\n",
    "\n",
    "        e += seq_length\n",
    "rnn = train_rnn(rnn, book_data, char_to_ind, ind_to_char, n_epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
